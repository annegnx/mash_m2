{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust methods for Machine Learning\n",
    "\n",
    "## Defenses against adversarial attacks\n",
    "\n",
    "#### Tutorial #3 (Anne Gagneux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attack(model, image, image_adv):\n",
    "    pred_orig = model(image)\n",
    "    pred_adv = model(image_adv)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    axs[0].set_title('Prediction orig: %s. Confidence orig: %d' %\n",
    "                     (pred_orig.argmax().item(), pred_orig.max()*100),\n",
    "                     fontsize=12)\n",
    "    axs[0].imshow((image).reshape((28, 28)).detach().numpy(),\n",
    "                  cmap=plt.cm.gray_r, vmin=0, vmax=max(min(255, image.max()), 1))\n",
    "    axs[1].set_title('Prediction attack: %s. Confidence attack: %d ' %\n",
    "                     (pred_adv.argmax().item(), pred_adv.max()*100),\n",
    "                     fontsize=12)\n",
    "    axs[1].imshow((image_adv).detach().numpy().reshape((28, 28)),\n",
    "                  cmap=plt.cm.gray_r, vmin=0, vmax=max(min(255, image.max()), 1))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A last adversarial attack for the road\n",
    "\n",
    "#### DeepFool: back to the linear model\n",
    "\n",
    "Remember the linear model $h(x) = w^T x + b$.\n",
    "For a given image $x_1$, the orthogonal projection problem writes as:\n",
    "$$\\min_x \\Vert x-x_1 \\Vert_2 \\text{ subject to } w^Tx+ b = 0$$\n",
    "\n",
    "We had the projection:\n",
    "\\begin{equation}\n",
    " \\mathbf x^* = \\mathbf x_1 - \\frac{w^T \\mathbf x_1 + b}{\\Vert w \\Vert^2} w\n",
    "\\end{equation}\n",
    "\n",
    "What if now we do not have a linear model but a deep neural network instead $h_\\theta$.\n",
    "We can just linearize it ! \n",
    "\n",
    "*Recall : Taylor's expansion* \n",
    "\n",
    "\\begin{align} \n",
    "h_\\theta(\\mathbf x) & \\approx h_\\theta(\\mathbf x_1) + \\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1)^T (\\mathbf x - \\mathbf x_1) \\\\ \n",
    "&  = \\underbrace{\\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1)^T}_{w^T} \\mathbf x + \\underbrace{(h_\\theta(\\mathbf x_1)  -\\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1)^T \\mathbf x_1)}_{b}\n",
    "\\end{align}\n",
    "\n",
    "We can write a new \"projection\"-like perturbed image as:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf x^* & = \\mathbf x_1 - \\frac{\\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1)^T \\mathbf x_1 + (h_\\theta(\\mathbf x_1)  -\\nabla_{\\mathbf x}   h_\\theta (\\mathbf x_1)^T \\mathbf x_1)}{\\Vert \\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1) \\Vert^2} \\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1) \\\\\n",
    " &=  \\mathbf x_1 - \\frac{ h_\\theta(\\mathbf x_1)  }{\\Vert \\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1) \\Vert^2} \\nabla_{\\mathbf x}  h_\\theta (\\mathbf x_1)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "![](figures/decision-boundary.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Colab\n",
    "# %matplotlib inline\n",
    "# img = plt.imread('/content/decision-boundary.jpg')\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool_binary(model, X, y, overshoot=0.2, max_iters=10):\n",
    "    image = X.float().detach().numpy().copy().reshape((1, 28, 28))\n",
    "    current_x = Variable(torch.from_numpy(image), requires_grad=True)\n",
    "    pred = model(current_x)\n",
    "    n_iter = 0\n",
    "    delta = np.zeros_like(image)\n",
    "    while pred.argmax() == y and n_iter <= max_iters:\n",
    "        pred[0, y].backward(retain_graph=True) \n",
    "        grad = current_x.grad.data.detach().numpy().copy()\n",
    "        delta -= # TO COMPLETE\n",
    "        current_x.grad.data.zero_()\n",
    "        current_x = Variable(torch.from_numpy(\n",
    "            image + (1+overshoot) * delta), requires_grad=True)\n",
    "        pred = model(current_x)\n",
    "    x_adv = torch.from_numpy()# TO COMPLETE\n",
    "    return x_adv.reshape(X.shape)\n",
    "\n",
    "\n",
    "def fgsm(model, X, y, epsilon):\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred = model(X+delta)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    x_adv = X + epsilon * torch.sign(delta.grad.detach())\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a Fully Connected Network on  binary MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST\n",
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "# Only keep 3 and 7\n",
    "train_idx = (train_data.targets == 0) + (train_data.targets == 1)\n",
    "\n",
    "train_data.data = train_data.data[train_idx]\n",
    "train_data.targets = train_data.targets[train_idx]\n",
    "\n",
    "test_idx = (test_data.targets == 0) + (test_data.targets == 1)\n",
    "test_data.data = test_data.data[test_idx]\n",
    "test_data.targets = test_data.targets[test_idx]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0005)\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    EPOCHS = 2\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(output.data, axis=1)\n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    correct = 0\n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        output = model(test_imgs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}%\".format(\n",
    "        float(correct*100) / (len(test_loader)*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "fc = FC()\n",
    "train(fc, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(fc, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = (test_data.data[test_data.targets == 1][1])\n",
    "image_adv = deepfool_binary(fc, image1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attack(fc, image1.float().reshape(1, 28, 28),\n",
    "               image_adv.float().reshape(1, 28, 28))\n",
    "print(\"Total variation pixels:\", np.sum(np.abs((image_adv-image1).numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = torch.tensor((test_data.data[test_data.targets == 1][1]).reshape(\n",
    "    1, 1, 28, 28), dtype=torch.float)\n",
    "image_adv = fgsm(fc, image1, torch.LongTensor([target]), 15)\n",
    "display_attack(fc, image1.float().reshape(1, 28, 28),\n",
    "               image_adv.float().reshape(1, 28, 28))\n",
    "print(\"Total variation pixels:\", np.sum(np.abs((image_adv-image1).numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class DeepFool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool(model, X, y, overshoot=0, max_iter=5):\n",
    "    image = X.detach().numpy().copy()\n",
    "    current_x = Variable(torch.from_numpy(image), requires_grad=True)\n",
    "    preds = model(current_x)\n",
    "    n_classes = preds.shape[-1]\n",
    "    preds_true_class = preds[0, y]\n",
    "    preds_true_class.backward(retain_graph=True)\n",
    "    grad_true_class = current_x.grad.data.detach().numpy().copy()\n",
    "    ratio = np.inf\n",
    "    delta = np.zeros_like(X)\n",
    "    n_iter = 0\n",
    "    while preds.argmax() == y and n_iter <= max_iter:\n",
    "        n_iter += 1\n",
    "        for k in range(n_classes):\n",
    "            if k != y:\n",
    "                current_x.grad.data.zero_()\n",
    "                preds_k = preds[0, k]\n",
    "                preds_k.backward(retain_graph=True)\n",
    "                grad_k = current_x.grad.data.detach().numpy().copy()\n",
    "                diff_grads = grad_k-grad_true_class\n",
    "                diff_preds = preds_true_class-preds_k\n",
    "                new_ratio = np.abs(diff_preds.detach().numpy()) / \\\n",
    "                    np.linalg.norm(diff_grads)**2\n",
    "                if new_ratio < ratio:\n",
    "                    ratio = new_ratio\n",
    "                    delta_iter = ratio * diff_grads\n",
    "        delta += delta_iter\n",
    "        current_x = Variable(torch.from_numpy(\n",
    "            image + (1+overshoot) * delta), requires_grad=True)\n",
    "        preds = model(current_x)\n",
    "        preds_true_class = preds[0, y]\n",
    "        preds_true_class.backward(retain_graph=True)\n",
    "        grad_true_class = current_x.grad.data.detach().numpy().copy()\n",
    "\n",
    "    x_adv = torch.from_numpy(image + (1+overshoot) * delta)\n",
    "    return x_adv.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a CNN on 10-classes MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "                                        nn.Conv2d(32, 32, 3, padding=1,\n",
    "                                                  stride=2), nn.ReLU(),\n",
    "                                        nn.Conv2d(\n",
    "                                            32, 64, 3, padding=1), nn.ReLU(),\n",
    "                                        nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU())\n",
    "        self.fc_layers = nn.Sequential(nn.Linear(7*7*64, 100), nn.ReLU(),\n",
    "                                       nn.Linear(100, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = (self.fc_layers(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=3):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cnn = CNN()\n",
    "train(cnn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "for n, (image, target) in enumerate(zip(images, labels)):\n",
    "    if n <= 10:\n",
    "        image_adv = deepfool(cnn, image.reshape(\n",
    "            (1, 1, 28, 28)),  torch.LongTensor([target]))\n",
    "\n",
    "        if cnn(image_adv).argmax() != target:\n",
    "            display_attack(cnn, image.reshape((1, 1, 28, 28)),\n",
    "                           image_adv.reshape((1, 1, 28, 28)))\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "for n, (image, target) in enumerate(zip(images, labels)):\n",
    "    if n <= 10:\n",
    "        image_adv = fgsm(cnn, image.reshape((1, 1, 28, 28)),\n",
    "                         torch.LongTensor([target]), 0.2)\n",
    "\n",
    "        if cnn(image_adv).argmax() != target:\n",
    "            display_attack(cnn, image.reshape((1, 1, 28, 28)),\n",
    "                           image_adv.reshape((1, 1, 28, 28)))\n",
    "\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have seen how an attacker could create adversarial examples which fool a classifier.\n",
    "The goal of the attacker can write as follows:\n",
    "$$ \\max_{\\delta \\in \\Delta} l (h_\\theta (\\mathbf x + \\delta ), y) $$\n",
    "where $\\Delta$ is the set of allowed perturbations, $\\mathbf x$ is the original image we want to attack, $\\delta$ is the perturbation, $h_\\theta$ denots the net trained with weights $\\theta$, $l$ is the loss used for training and $y$ the ground-truth label.\n",
    "\n",
    "To defend against these attacks, one way is to include adversarial examples into the training process.\n",
    "\n",
    "- The usual training optimization problem writes as:\n",
    "$$\\min_\\theta \\frac{1}{N}\\sum_{(\\mathbf x,y) \\in \\mathcal D_N} l (h_\\theta ( \\mathbf x),y)$$\n",
    "\n",
    "Which we solve with the following iterations:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\sum_{(\\mathbf x,y) \\in \\mathcal B} \\nabla_\\theta l (h_\\theta(\\mathbf x), y) $$\n",
    "\n",
    "- The *adversarial* training optimization problem writes as:\n",
    "\n",
    "$$\\min_\\theta \\frac{1}{N}\\sum_{(\\mathbf  x,y) \\in \\mathcal D_N}  \\max_{\\delta \\in \\Delta}  l (h_\\theta ( \\mathbf x + \\delta),y)$$\n",
    "\n",
    "$\\max_{\\delta}$ means that we want to anticipate the worst-case attack. \n",
    "\n",
    "The iterations **we would like to compute**:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\sum_{(\\mathbf  x,y) \\in \\mathcal B} \\nabla_\\theta \\left[ \\max_{\\delta \\in \\Delta} l (h_\\theta( \\mathbf x + \\delta), y) \\right]$$\n",
    "\n",
    "**BUT how ?**\n",
    "\n",
    "*Danskin's theorem* Under suitable conditions, one has:\n",
    "$$ \\nabla_\\theta \\left[ \\max_{\\delta \\in \\Delta} l (h_\\theta(\\mathbf x + \\delta), y) \\right] = \\nabla_\\theta \\left[  l (h_\\theta(\\mathbf x + \\delta^* ), y) \\right] \\quad  \\text{where } \\delta^* = \\arg\\max_{\\delta \\in \\Delta} (h_\\theta(x+\\delta),y)$$\n",
    "\n",
    "In other words (under suitable conditions), to evaluate the gradient of the supremum of a class of functions, one should simply evaluate the gradient of the function in the class that actually obtains the maximum (if one exists).\n",
    "This means that if we want to train a robust network using stochastic gradient descent (SGD), we just need to train it maximally perturbed images. Sadly, we don't know how to find these images exactly. \n",
    "\n",
    "$\\rightarrow$ The good news: we can try to approximate these \"maximally\" perturbed images with the ones we know how to compute (FGSM, projected gradient descent, DeepFool, etc) i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "# data loader\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_Linfty(x, epsilon):\n",
    "    return x.clamp(-epsilon, epsilon)\n",
    "\n",
    "\n",
    "def projection_L2(x, epsilon):\n",
    "    x = x.cpu().detach().numpy().copy()\n",
    "    x = x / np.maximum(np.linalg.norm(x, axis=(2, 3)) /\n",
    "                       epsilon, 1)[:, :, None, None]\n",
    "    return Variable(torch.from_numpy(x).to(device), requires_grad=True)\n",
    "\n",
    "\n",
    "def pgd_Linfty(model, X, y, epsilon, n_steps=40, eta=0.01):\n",
    "    \"\"\" Run projected gradient descent on the examples X with set of pertubations allowed in Linfty norm\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    for k in range(n_steps):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pred = model(X+delta)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        delta.data = projection_Linfty(\n",
    "            delta.data + eta * delta.grad.detach().sign(), epsilon)\n",
    "        delta.grad.zero_()  # do not forget to put the gradient back to 0 before the next step\n",
    "    return X + delta.detach()\n",
    "\n",
    "\n",
    "def pgd_L2(model, X, y, epsilon, n_steps=40, eta=0.1):\n",
    "    \"\"\" Run projected gradient descent on the examples X with set of pertubations allowed in L2 norm\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    for k in range(n_steps):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pred = model(X+delta)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        grad = delta.grad.detach().cpu().numpy().copy()\n",
    "        grad /= np.linalg.norm(grad, axis=(2, 3))[:, :, None, None]\n",
    "\n",
    "        delta.data = projection_L2(\n",
    "            delta.data + eta * torch.from_numpy(grad).to(device), epsilon)\n",
    "        delta.grad.zero_()  # do not forget to put the gradient back to 0 before the next step\n",
    "    return X + delta.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(loader, model, opt=None):\n",
    "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
    "    total_loss, total_err = 0., 0.\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        yp = model(X)\n",
    "        loss = nn.CrossEntropyLoss()(yp, y)\n",
    "        if opt:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "dict_of_attacks = {\n",
    "    \"deepfool\": deepfool,\n",
    "    \"fgsm\": fgsm,\n",
    "    \"pgd_linfty\": pgd_Linfty,\n",
    "    \"pgd_l2\": pgd_L2,\n",
    "}\n",
    "\n",
    "\n",
    "def epoch_adversarial(loader, model, attack, opt=None, **kwargs):\n",
    "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
    "    total_loss, total_err = 0., 0.\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        if attack == \"deepfool\":  # does not support batch\n",
    "            model = model.cpu()\n",
    "            X, y = X.cpu(), y.cpu()\n",
    "            for i, (image, target) in enumerate(zip(X, y)):\n",
    "                image_adv = dict_of_attacks[attack](model, image.reshape(\n",
    "                    (1, 1, 28, 28)), target, **kwargs)\n",
    "                yp = model(image_adv)\n",
    "                loss = nn.CrossEntropyLoss()(yp, torch.LongTensor([target]))\n",
    "                if opt:\n",
    "                    opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "                total_err += (yp.max(dim=1)[1] != target).item()\n",
    "                total_loss += loss.item()\n",
    "        else:\n",
    "            X_adv = dict_of_attacks[attack](model, X, y, **kwargs)\n",
    "            yp = model(X_adv)\n",
    "            loss = nn.CrossEntropyLoss()(yp, y)\n",
    "            if opt:\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a standart CNN and evaluate it under an attacked dataset.\n",
    "What do you observe ? Is it robust ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "standartCNN = CNN()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    standartCNN.cuda()\n",
    "\n",
    "opt = optim.SGD(standartCNN.parameters(), lr=1e-1)\n",
    "for t in range(2):\n",
    "    train_err, train_loss = epoch(train_loader, standartCNN, opt)\n",
    "    test_err, test_loss = epoch(test_loader, standartCNN)\n",
    "    adv_err, adv_loss = epoch_adversarial(\n",
    "        test_loader, standartCNN, \"pgd_linfty\", epsilon=0.1)\n",
    "    if t == 4:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-2\n",
    "    print(*(\"{:.6f}\".format(i)\n",
    "          for i in (train_err, test_err, adv_err)), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a robust CNN and evaluate it under an attacked dataset.\n",
    "What do you observe ? Is it robust ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "robustCNN = CNN()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    robustCNN.cuda()\n",
    "\n",
    "\n",
    "opt = optim.SGD(robustCNN.parameters(), lr=1e-1)\n",
    "for t in range(2):\n",
    "    train_err, train_loss = # TO COMPLETE\n",
    "    test_err, test_loss = # TO COMPLETE\n",
    "    adv_err, adv_loss = # TO COMPLETE\n",
    "    if t == 4:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-2\n",
    "    print(*(\"{:.6f}\".format(i)\n",
    "          for i in (train_err, test_err, adv_err)), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can evaluate you robust CNN against various attacks ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FGSM: \", epoch_adversarial(\n",
    "    test_loader, robustCNN, \"fgsm\", epsilon=0.2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FGSM: \", epoch_adversarial(\n",
    "    test_loader, standartCNN, \"fgsm\", epsilon=0.2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"PGD, projection $\\ell_2$: \", epoch_adversarial(\n",
    "    test_loader, robustCNN, \"pgd_l2\", epsilon=0.2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"PGD, projection $\\ell_2$: \", epoch_adversarial(\n",
    "    test_loader, robustCNN, \"pgd_l2\", epsilon=0.4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"DeepFool\", epoch_adversarial(\n",
    "    test_loader, robustCNN, \"deepfool\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"PGD, projection $\\ell_2$: \", epoch_adversarial(\n",
    "    test_loader, robustCNN, \"pgd_linfty\", epsilon=0.4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on you results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
