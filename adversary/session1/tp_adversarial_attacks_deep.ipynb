{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust methods for Machine Learning\n",
    "\n",
    "## Adversarial attacks\n",
    "\n",
    "#### Tutorial #2 (Anne Gagneux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to consider a neural network $h_\\theta$ (for classification) that has been trained on  $\\mathbf X_{\\text{train}}$ with the corresponding labels $y_{\\text{train}}$. The general idea for *adversarial attacks* is to create $\\mathbf x^{adv}$ by adding a small pertubation that changes the prediction. \n",
    "\n",
    "What is the intuition behind adversarial attacks ? How to explain that classifiers have both such good performance and yet fail when one adds little perturbations?\n",
    "\n",
    "One of the hypotheses can be summed up as follows:\n",
    "\n",
    "> Clean data lies in a low-dimensional manifold. Even though the adversarial examples are close to the clean data, they lie off the underlying data manifold.\n",
    "\n",
    "In other words, the neural network learns the underlying (hidden) manifold of real-life images. Even small perturbations get the sample out of this manifold that is totally unknown to the neural network (and unseen during training). \n",
    "\n",
    "![Image manifold](https://www.researchgate.net/publication/347339813/figure/fig1/AS:1020696654249985@1620364454164/We-illustrate-the-notion-of-a-hidden-manifold-in-input-space-using-CIFAR10-example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load an image that we are going to misclassify. We use ImageNet pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the image, resize to 224 and convert to PyTorch Tensor\n",
    "img = Image.open(\"beagle.jpg\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "beagle_image = preprocess(img)[None, :, :, :]\n",
    "\n",
    "# plot image (note that numpy using HWC whereas Pytorch user CHW, so we need to convert)\n",
    "plt.imshow(beagle_image[0].numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load a classification model (Resnet50) pretrained on ImageNet (1000 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Module to normalize an image\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.Tensor(mean)\n",
    "        self.std = torch.Tensor(std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None, :, None, None]) / self.std.type_as(x)[None, :, None, None]\n",
    "\n",
    "\n",
    "# values are standard normalization for ImageNet images,\n",
    "# from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# load pre-trained ResNet50, and put into evaluation mode (necessary to e.g. turn off batchnorm)\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict the class of our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(norm(beagle_image))\n",
    "with open(\"imagenet_class_index.json\") as f:\n",
    "    imagenet_classes = {int(i): x[1] for i, x in json.load(f).items()}\n",
    "print(imagenet_classes[pred.argmax().item()])\n",
    "beagle_class = pred.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_attack(model, image, image_adv):\n",
    "    \"\"\" For a given attack, create an adversarial image and compare the predictions of the model on the original image and the adversarial one\"\"\"\n",
    "\n",
    "    pred = model(norm(image))\n",
    "    max_class = pred.argmax().item()\n",
    "    print(\"Predicted class of before attack: \", imagenet_classes[max_class])\n",
    "    print(\"Predicted probability of true class before attack: \",\n",
    "          nn.Softmax(dim=1)(pred)[0, max_class].item())\n",
    "\n",
    "    pred_adv = model(norm(image_adv))\n",
    "    max_class_adv = pred_adv.argmax().item()\n",
    "    print(\"\")\n",
    "    print(\"Predicted class after attack: \", imagenet_classes[max_class_adv])\n",
    "    print(\"Predicted probability of true class after attack\", nn.Softmax(\n",
    "        dim=1)(pred_adv)[0, max_class].item())\n",
    "    print(\"Predicted probability of new class\", nn.Softmax(\n",
    "        dim=1)(pred_adv)[0, max_class_adv].item())\n",
    "\n",
    "    print(\"\")\n",
    "    if max_class_adv != max_class:\n",
    "        print(\"The attack was successful !\")\n",
    "    else:\n",
    "        print(\"The attack did not succeed.\")\n",
    "\n",
    "\n",
    "def perform_target_attack(model, image, image_adv, y_target):\n",
    "\n",
    "    pred = model(norm(image))\n",
    "    max_class = pred.argmax().item()\n",
    "    print(\"Predicted class of before attack: \", imagenet_classes[max_class])\n",
    "    print(\"Predicted probability of true class before attack: \",\n",
    "          nn.Softmax(dim=1)(pred)[0, max_class].item())\n",
    "    print(\"Predicted probability of target class before attack: \",\n",
    "          nn.Softmax(dim=1)(pred)[0, y_target].item())\n",
    "\n",
    "    pred_adv = model(norm(image_adv))\n",
    "    max_class_adv = pred_adv.argmax().item()\n",
    "\n",
    "    print(\"\")\n",
    "    if max_class_adv == y_target:\n",
    "        print(\"The attack was successful !\")\n",
    "        print(\"\")\n",
    "        print(\"Predicted class after attack: \",\n",
    "              imagenet_classes[max_class_adv])\n",
    "        print(\"Predicted probability of true class after attack\", nn.Softmax(\n",
    "            dim=1)(pred_adv)[0, max_class].item())\n",
    "        print(\"Predicted probability of target class\", nn.Softmax(\n",
    "            dim=1)(pred_adv)[0, max_class_adv].item())\n",
    "    else:\n",
    "        print(\"The attack did not succeed.\")\n",
    "        print(\"\")\n",
    "        print(\"Predicted class after attack: \",\n",
    "              imagenet_classes[max_class_adv])\n",
    "        print(\"Predicted probability of true class after attack\", nn.Softmax(\n",
    "            dim=1)(pred_adv)[0, max_class].item())\n",
    "        print(\"Predicted probability of target class\", nn.Softmax(\n",
    "            dim=1)(pred_adv)[0, y_target].item())\n",
    "        print(\"Predicted probability of adv class\", nn.Softmax(\n",
    "            dim=1)(pred_adv)[0, max_class_adv].item())\n",
    "\n",
    "\n",
    "def display_attack(image, image_adv):\n",
    "    \"\"\"Display the adversarial example for a given attack\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow((image)[0].detach().numpy().transpose(1, 2, 0))\n",
    "    axs[1].imshow((image_adv)[0].detach().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_diff(image, image_adv):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    diff = (image_adv-image)[0].detach()\n",
    "    plt.imshow((torch.abs(diff)/(torch.max(diff)-torch.min(diff))\n",
    "                ).numpy().transpose(1, 2, 0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(r\"$|x - x^{adv}|$\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"White-box\" attacks\n",
    "White-box attacks mean we have *access to the full network*. Our goal is to slightly modify the image, in such a way that a human does not notice it but that a neural network is no longer capable of correctly classifying the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random pertubation\n",
    "Random pertubation is the most trivial attack. One can randomly perturb the image pixel by pixel within a given range.\n",
    "It writes as:\n",
    "$$ \\mathbf x^{adv} = \\mathbf x + \\mathcal U([-\\epsilon, \\epsilon])$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_perturbation(X, epsilon):\n",
    "    \"\"\" Construct randomly pertubed adversarial examples on the examples X\"\"\"\n",
    "    delta = # TO COMPLETE\n",
    "    x_adv = X + delta\n",
    "    return x_adv.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the attack\n",
    "epsilon = # TO COMPLETE: epsilon in [0,1]. The greater the epsilon, the more visible the pertubation becomes. Try different values.\n",
    "image_adv = random_perturbation(beagle_image, epsilon=epsilon)\n",
    "perform_attack(model, beagle_image, image_adv)\n",
    "display_attack(beagle_image, image_adv)\n",
    "display_diff(beagle_image, image_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient based adversarial attacks\n",
    "\n",
    "When training a neural network, ones minimize the loss $\\ell(\\mathbf X_{\\text{train}}, y_{\\text{train}})$ using back-propagation. \\\n",
    "These gradients are used to update the weights of the neural network. \\\n",
    "Now, we consider a trained neural network. Here, we want to maximize the loss (in order to force misclassification). \\\n",
    "Instead of updating the weights, we want to update the image itself, i.e. tune the pertubation $\\delta$ in order to misclassify the image $\\mathbf x$.\n",
    "\n",
    "*Notations:*\n",
    "In the following $h_\\theta$ denotes the model with weights $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fast gradient sign method (FGSM)\n",
    "\n",
    "The idea is to adjust the pertubation $\\delta$ in the direction of the gradient in order to maximise the loss. \\\n",
    "One wish to do the larger step as possible, so to always move by $\\pm \\epsilon$.\n",
    "\n",
    "It writes as:\n",
    "$$ \\mathbf x^{adv} = \\mathbf x + \\epsilon \\cdot \\mathrm{sign} \\left( \\nabla_{\\mathbf x} \\ell (\\mathbf x, y) \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(model, X, y, epsilon):\n",
    "    \"\"\" Construct randomly pertubed adversarial examples on the examples X\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred = model(norm(X+delta))\n",
    "    loss = # TO COMPLETE \n",
    "    loss.backward()\n",
    "    x_adv = # TO COMPLETE\n",
    "    return x_adv.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the attack\n",
    "image_adv = fgsm(model, beagle_image, beagle_class, epsilon=10./255.)\n",
    "perform_attack(model, beagle_image, image_adv)\n",
    "display_attack(beagle_image, image_adv)\n",
    "display_diff(beagle_image, image_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projected gradient descent (PGD)\n",
    "\n",
    "We write tha attack as a constrained optimization problem:\n",
    "$$ \\max_{\\delta \\in \\Delta} l(h_\\theta(\\mathbf x+\\delta),y) $$\n",
    "\n",
    "**PGD iterations:**\n",
    "$$\\delta_{t+1} = \\mathcal P_\\Delta ( \\delta_t + \\eta_t \\nabla_\\delta l(h_\\theta(\\mathbf x +\\delta_t,y)) )$$\n",
    "\n",
    "One needs to compute the projection onte the set $\\Delta$.\n",
    "\n",
    "- If $\\Delta$ is the $\\ell_\\infty$ ball, i.e. $\\Delta = \\mathcal B_\\infty (0,\\epsilon) = \\{ \\mathbf x : \\max_i {|x_i|} \\leq \\epsilon\\}$, the projection simply writes as  clipping values to $[-\\epsilon, \\epsilon]$.\n",
    "\n",
    "- If $\\Delta$ is the $\\ell_2$ ball, i.e. $\\Delta = \\mathcal B_2 (0,\\epsilon) = \\{ \\mathbf x : \\Vert x \\Vert_2 \\leq \\epsilon\\}$, the projection writes as:\n",
    "$$ \\mathcal P_{\\ell_2} ( a )  = \\frac{a}{ \\max \\left( \\frac{\\Vert a \\Vert}{\\epsilon} , 1 \\right)} $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_Linfty(x, epsilon):\n",
    "    # TO COMPLETE\n",
    "    return None\n",
    "\n",
    "\n",
    "def projection_L2(x, epsilon):\n",
    "    # TO COMPLETE\n",
    "    return None\n",
    "\n",
    "\n",
    "def pgd(model, X, y, epsilon, projection, n_steps=40, eta=5e-3):\n",
    "    \"\"\" Run projected gradient descent on the examples X\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    for k in trange(n_steps):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pred = model(norm(X+delta))\n",
    "        loss = # TO COMPLETE\n",
    "        loss.backward()\n",
    "        delta.data = # TO COMPLETE\n",
    "        delta.grad.zero_()  # do not forget to put the gradient back to 0 before the next step\n",
    "    x_adv = X + delta\n",
    "    return x_adv.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the attack\n",
    "image_adv = pgd(model, beagle_image, beagle_class,\n",
    "                epsilon=0.5, projection=projection_L2)\n",
    "perform_attack(model, beagle_image, image_adv)\n",
    "display_attack(beagle_image, image_adv)\n",
    "display_diff(beagle_image, image_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the attack\n",
    "image_adv = pgd(model, beagle_image, beagle_class,\n",
    "                epsilon=10./255., projection=projection_Linfty, eta=0.1)\n",
    "perform_attack(model, beagle_image, image_adv)\n",
    "display_attack(beagle_image, image_adv)\n",
    "display_diff(beagle_image, image_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go further, you can try with $\\Delta$ the $\\ell_1$ ball, i.e. $\\Delta = \\mathcal B_1 (0,\\epsilon) = \\{ \\mathbf x : \\sum_i |x_i|\\leq \\epsilon\\}$. You first need to compute the projection operator (you can  write the Lagrangian and use KKT conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_L1(x, epsilon):\n",
    "    original_shape = x.shape\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    mask = (torch.norm(x, p=1, dim=1) < epsilon).float().unsqueeze(1)\n",
    "    mu, _ = torch.sort(torch.abs(x), dim=1, descending=True)\n",
    "    cumsum = torch.cumsum(mu, dim=1)\n",
    "    arange = torch.arange(1, x.shape[1] + 1, device=x.device)\n",
    "    rho, _ = torch.max((mu * arange > (cumsum - epsilon)) * arange, dim=1)\n",
    "    theta = (cumsum[torch.arange(x.shape[0]), rho.cpu() - 1] - epsilon) / rho\n",
    "    proj = (torch.abs(x) - theta.unsqueeze(1)).clamp(min=0)\n",
    "    x = mask * x + (1 - mask) * proj * torch.sign(x)\n",
    "    return x.view(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the attack\n",
    "image_adv = pgd(model, beagle_image, beagle_class,\n",
    "                epsilon=0.2, projection=projection_L1, eta=5e-3)\n",
    "perform_attack(model, beagle_image, image_adv)\n",
    "display_attack(beagle_image, image_adv)\n",
    "display_diff(beagle_image, image_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe ? Look at the average difference between the original image and the target image ? What can you say ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.abs(beagle_image-image_adv)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targeted attack\n",
    "\n",
    "What we have considered so far are “untargeted” attacks, meaning they effectively try to change the label to any alternative, rather than change it to a particular alternative. If we want to target the attack to a specific laabel, we can both maximize the loss on the true label and minimize the loss on the target label.\n",
    "\n",
    "It writes:\n",
    "\n",
    "$$ \\max_{\\delta \\in \\Delta} l(h_\\theta(\\mathbf x+\\delta),y_{\\text{truth}}) - l(h_\\theta(\\mathbf x+\\delta),y_{\\text{target}})$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_target(model, X, y, y_target, epsilon):\n",
    "    \"\"\" Construct randomly pertubed adversarial examples on the examples X\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred = model(norm(X+delta))\n",
    "    loss = # TO COMPLETE\n",
    "    loss.backward()\n",
    "    x_adv = # TO COMPLETE\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_target(model, X, y, y_target, epsilon, projection, n_steps=100, eta=5e-3):\n",
    "    \"\"\" Run projected gradient descent on the examples X\"\"\"\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    \n",
    "    for k in trange(n_steps):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pred = model(norm(X+delta))\n",
    "        loss = # TO COMPLETE\n",
    "        loss.backward()\n",
    "        delta.data = # TO COMPLETE\n",
    "        delta.grad.zero_()  # do not forget to put the gradient back to 0 before the next step\n",
    "    x_adv = X + delta\n",
    "    return x_adv.clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a class of your choice looking in the imagenet_class_index.json.\n",
    "Try to change the beagle_image for the net to predict with your chosen class. \n",
    "Try for different methods, different classes, different range of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the attack\n",
    "image_adv = pgd_target(model, beagle_image, beagle_class, y_target=330,\n",
    "                       epsilon=0.2, projection=projection_Linfty)\n",
    "perform_target_attack(model, beagle_image, image_adv, 330)\n",
    "display_attack(beagle_image, image_adv)\n",
    "display_diff(beagle_image, image_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-box attack\n",
    "\n",
    "Here, we do not have access to the weights of the model anymore. \n",
    "Thus, we can not perform back-propagation to find the best perturbation. \\\n",
    "In the black-box setting, an attacker can only access outputs of the target model. Based on whether one has access to the full probability or the label of a given input, black-box attacks are further divided into score-based and decision-based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HopSkipJump Attack: a decision-based adversarial attack\n",
    "\n",
    "*Reference: [HopSkipJumpAttack: A Query-Efficient Decision-Based Adversarial Attack](https://arxiv.org/abs/1904.02144) by Jianbo Chen, Michael I. Jordan, Martin J. Wainwright.*\n",
    "\n",
    "We suppose we have access to the output labels of the model we want to attack, i.e. we only have access to the prediction of an input $\\mathbf x$. It is the most restrictive setting.\n",
    "\n",
    "We do not have access to the gradient: the HopSkipJump attacks proposes to estimate this gradient.\n",
    "\n",
    "Let $m$ be the number of classes.\n",
    "We want the net to misclassify our input $\\mathbf x$.\n",
    "So, if $c = \\arg \\max_{i \\in [1,m]} h_\\theta(\\mathbf x)$ is the predicted class, we want that:\n",
    "$$\\arg \\max_{i \\in [1,m]} h_\\theta(\\mathbf x^{adv}) \\neq c$$\n",
    "which rewrites as:\n",
    "$$S_{\\mathbf x} (\\mathbf x^{adv})  = \\max_i  [ h_\\theta(\\mathbf x^{adv})]_i - [ h_\\theta(\\mathbf x^{adv})]_c > 0 $$\n",
    "\n",
    "\n",
    "The goal of an adversarial attack is to generate a perturbed sample $\\mathbf x^{adv}$ such that $S_{\\mathbf x} (\\mathbf x^{adv}) > 0$, while keeping $\\mathbf x^{adv}$ close to the original sample $\\mathbf x$. \n",
    "This can be formulated as the optimization problem:\n",
    "$$ \\min_{\\mathbf x'} d(\\mathbf x', \\mathbf x) \\text{ such that } S_{\\mathbf x} (\\mathbf x') > 0$$\n",
    "\n",
    "For the $\\ell_2$ distance:\n",
    "$$d(\\mathbf x', \\mathbf x) = \\Vert x- x' \\Vert_2$$\n",
    "\n",
    "We are given an image $\\mathbf x^*$ we want to attack (i.e. to modify slightly for the model to make a wrong prediction).\n",
    "The algorithm takes an other misclassified image $x_0$ such that $S_{\\mathbf x} (x_0) > 0$ and updates its pixels in the following iterative way:\n",
    "\n",
    "$$ \\mathbf x_{t+1} = \\alpha_t \\mathbf x^* + (1 - \\alpha_t) (\\mathbf x_t + \\eta \\nabla S_{\\mathbf x} (\\mathbf x_t) ) $$\n",
    "\n",
    "**BUT how to compute $ \\nabla S_{\\mathbf x} (\\mathbf x_t)$ ?**\n",
    "\n",
    "Monte-Carlo estimate at the boundary (when $S_{\\mathbf x} (\\mathbf x_t) \\sim 0$):\n",
    "\n",
    "$$\\widetilde{\\nabla S_{\\mathbf x}} (\\mathbf x_t) := \\frac{1}{B} \\sum_{b=1}^B \\mathbf 1_{S_{\\mathbf x} (\\mathbf x_t + \\delta u_b)>0} u_B$$\n",
    "where $\\{ u_b \\}_{b=1}^B$ are i.i.d. draws from the uniform distribution over the unit sphere.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(model, norm, X, y, thresh=0.001):\n",
    "    # Find a misclassified random noise.\n",
    "    success = 0\n",
    "    while not success:\n",
    "        random_noise = torch.rand(X.shape)\n",
    "        success = torch.argmax(model(norm((random_noise)))).item() != y\n",
    "\n",
    "    # Binary search to minimize l2 distance to original image.\n",
    "    alpha_low = 0.0\n",
    "    alpha_up = 1.0\n",
    "    while np.abs(alpha_low - alpha_up) > thresh:\n",
    "        alpha_med = 0.5 * (alpha_low + alpha_up)\n",
    "        tmp_x = (1 - alpha_med) * X + alpha_med * random_noise\n",
    "        success = torch.argmax(model(norm((tmp_x)))).item() != y\n",
    "        if success:\n",
    "            alpha_up = alpha_med\n",
    "        else:\n",
    "            alpha_low = alpha_med\n",
    "\n",
    "    initialization = (1 - alpha_up) * X + alpha_up * random_noise\n",
    "\n",
    "    return initialization\n",
    "\n",
    "\n",
    "def bin_search(model,  norm, X, currentX, y,  eta, grad, thresh=0.001):\n",
    "    \"\"\"TO COMPLETE: approach the boundary via a binary search.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def montecarlo_grad(model,  norm, X, y, B, delta):\n",
    "    \"\"\" TO COMPLETE: estimate the gradient\"\"\"\n",
    "    grad /= torch.norm(grad, p=2)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def hopskipjump(model,  norm, X, x0, y, n_steps=50, B0=100):\n",
    "    \"\"\" Run Hop Skip Jump for L2 distance in an untargeted setting\"\"\"\n",
    "    current_x = x0\n",
    "    d = torch.tensor(int(np.prod(X.shape)))\n",
    "    theta = 1.0 / (np.sqrt(d) * d)\n",
    "    for k in trange(1, n_steps+1):\n",
    "        dist = torch.norm(current_x-X, p=2)\n",
    "        if k == 1:\n",
    "            delta = 0.1\n",
    "        else:\n",
    "            delta = torch.sqrt(d) * theta * dist\n",
    "        B = int(B0 * np.sqrt(k))\n",
    "        grad = montecarlo_grad(model, norm, X, y, B, delta)\n",
    "        eta = dist / np.sqrt(k)\n",
    "        success = torch.argmax(model(norm(current_x + eta * grad))).item() != y\n",
    "        while success:\n",
    "            eta /= 2\n",
    "            success = torch.argmax(\n",
    "                model(norm(current_x + eta * grad))).item() != y\n",
    "        current_x = bin_search(model, norm, X, current_x, y, eta, grad)\n",
    "    return current_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a CNN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.view(-1, 3*3*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.Tensor(mean)\n",
    "        self.std = torch.Tensor(std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x  # no normalization needed\n",
    "\n",
    "\n",
    "# values are standard normalization for ImageNet images,\n",
    "# from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "norm = Normalize(mean=[0], std=[1])\n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    # ,lr=0.001, betas=(0.9,0.999))\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    EPOCHS = 1\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    correct = 0\n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output, 1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}%\".format(\n",
    "        float(correct*100) / (len(test_loader)*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cnn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the black-box attack on the trained CNN\n",
    "image3 = (train_data.data[train_data.targets == 3][0]).reshape(1, 1, 28, 28)\n",
    "print(\"Predicted label before attack: \", cnn(\n",
    "    torch.tensor(image3, dtype=torch.float)).argmax().detach().item())\n",
    "\n",
    "\n",
    "x0 = initialize(cnn, norm, image3, 3)\n",
    "print(\"Predicted label of adversarial init before HopSkipJump: \",\n",
    "      cnn(x0).argmax().detach().item())\n",
    "display_attack(image3, x0)\n",
    "display_diff(image3, x0)\n",
    "\n",
    "image_adv = hopskipjump(cnn, norm,  image3, x0, 3)\n",
    "print(\"Predicted label of adversarial example after HopSkipJump: \",\n",
    "      cnn(image_adv).argmax().detach().item())\n",
    "display_attack(image3, image_adv)\n",
    "display_diff(image3, image_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
