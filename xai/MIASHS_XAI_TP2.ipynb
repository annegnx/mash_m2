{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqfOC2r-8PiE"
      },
      "source": [
        "# XAI - TP2: Visualizations for Neural Networks\n",
        "### Anne Gagneux\n",
        "\n",
        "\n",
        "**Topic**:\n",
        "Deep Learning models are considered “black box” models, i.e. we can't say much about how the neural network makes its prediction. The goal of this TP is to be able to determine, for a classification task, which parts of the image influence the prediction, in order to explain how the NN behave."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPVxs_7m6VX2"
      },
      "outputs": [],
      "source": [
        "# useful librairies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import urllib.request\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9hJSIIP6D22"
      },
      "source": [
        "\n",
        "## Pixel attribution (Saliency Maps)\n",
        "\n",
        "**Goal**: Highlight the pixels that were important in an image for the neural network prediction.\n",
        "\n",
        "**How does it work?**: It is called a *gradient-based method*: we will compute the gradient of the prediction with respect to the input features (i.e. the pixels).\n",
        "The general idea is that if a slight change in a pixel impacts a lot the prediction (i.e. if the aboluste value of the gradient with respect to this pixel is large), then this pixel is relevant for the prediction.\n",
        "\n",
        "**Method**:\n",
        "1. Compute the prediction (i.e. *the forward pass*).\n",
        "2. Compute the gradient of the class score of interest with respect to the input pixels. The gradients are set to zero for all classes except the desired class, which is set to 1.\n",
        "3. Vizualize the gradients.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXN0M6HF7hyZ"
      },
      "source": [
        "### Download the Model\n",
        "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
        "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
        "* **ResNet-34**: A deep architecture for image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFvzYtKe7g03",
        "outputId": "b7b9bb10-b755-4db6-eaaf-17e48c5606fb"
      },
      "outputs": [],
      "source": [
        "resnet34 = models.resnet34(pretrained=True)\n",
        "resnet34.eval()  # set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAiQyfA7x4T"
      },
      "source": [
        "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
        "\n",
        "\n",
        "Input image must be of size (3x224x224).\n",
        "\n",
        "First convolution layer with maxpool.\n",
        "Then 4 ResNet blocks.\n",
        "\n",
        "Output of the last ResNet block is of size (512x7x7).\n",
        "\n",
        "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx5Q7E6l4VM_",
        "outputId": "d9ecb02f-47a7-4a75-d6bf-427cba0963fc"
      },
      "outputs": [],
      "source": [
        "classes = pickle.load(urllib.request.urlopen(\n",
        "    'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
        "\n",
        "# classes is a dictionary with the name of each class\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsLE6Gvc8Pn5"
      },
      "source": [
        "### Input Images\n",
        "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
        "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgyZYdFL8DTp"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(dir_path):\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),  # resize the image to 224x224\n",
        "        transforms.ToTensor(),  # convert numpy.array to tensor\n",
        "        normalize]))  # normalize the tensor\n",
        "\n",
        "    return (dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2jCmSWH8Fwb",
        "outputId": "c56d0120-c8d7-4cde-90f3-e92ed49d9505"
      },
      "outputs": [],
      "source": [
        "# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n",
        "# otherwise the function won't find them\n",
        "\n",
        "import os\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"data/TP2_images\")\n",
        "!cd data/TP2_images && wget \"https://is1-ssl.mzstatic.com/image/thumb/Purple3/v4/50/0d/5d/500d5dad-2ddd-4556-4def-4b629a4b0ec0/source/256x256bb.jpg\" && wget \"https://www.strydomstud.com/s3.amazonaws.com/static.strydomstud.co.za/files/posts/339/images/medium3374.jpg\"\n",
        "dir_path = \"data/\"\n",
        "dataset = preprocess_image(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "eZ3fm1NP8Vol",
        "outputId": "1907ff90-90c2-45f9-973b-77a43f86201c"
      },
      "outputs": [],
      "source": [
        "# show the orignal image\n",
        "index = 1\n",
        "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
        "plt.imshow(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVNXJDYa8Ybu",
        "outputId": "68cb0c92-8aec-4ab2-ddfa-d74141517854"
      },
      "outputs": [],
      "source": [
        "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
        "values, indices = torch.topk(output, 3)\n",
        "print(\"Top 3-classes:\", indices[0].numpy(),\n",
        "      [classes[x] for x in indices[0].numpy()])\n",
        "print(\"Raw class scores:\", values[0].detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7aCRMrv8bEm"
      },
      "outputs": [],
      "source": [
        "def SaliencyMap( model, image, my_class_of_interest):\n",
        "  model.eval() # make sure the model is in eval mode\n",
        "\n",
        "  #we want to calculate gradient of higest score w.r.t. input\n",
        "  #so set requires_grad to True for input\n",
        "  image.requires_grad = True\n",
        "\n",
        "  # compute the output (i.e. the prediction of the models)\n",
        "  output = # TO COMPLETE\n",
        "\n",
        "  # backpropagate to compute the gradient with respect to the input\n",
        "  # TO COMPLETE\n",
        "\n",
        "  saliency_map = # TO COMPLETE\n",
        "\n",
        "  # renormalize between 0 and 1\n",
        "  saliency_map = # TO COMPLETE\n",
        "  return saliency_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHn1T0oB_XM2",
        "outputId": "9ceab8d1-2444-46c5-c93c-7ebf7b952267"
      },
      "outputs": [],
      "source": [
        "my_image = dataset[-1][0].view(1, 3, 224, 224)\n",
        "pixel_attrib_map = SaliencyMap(resnet34, my_image, 382)\n",
        "print(pixel_attrib_map.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "crafH9Zd_fd9",
        "outputId": "f9553a9b-b5bd-445f-e56d-107f2e99adc8"
      },
      "outputs": [],
      "source": [
        "input_image = transforms.CenterCrop(224)(\n",
        "    Image.open(dataset.imgs[index][0]).convert('RGB'))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(input_image)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(pixel_attrib_map.detach().numpy(), cmap=plt.cm.hot)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h63T7hwUDWOa"
      },
      "source": [
        "\n",
        "## GradCAM\n",
        "\n",
        "**Goal**: Highlight *regions* of pixels that were important in an image for the neural network prediction. The goal of GradCAM is to understand at which parts of an image a convolutional layer “looks” for a certain classification.\n",
        "\n",
        "**How does it work?**: Here, the gradient is not backpropagated all the way back to the image, but to the last convolutional layer to produce a coarse localization map that highlights important regions of the image.\n",
        "\n",
        "There are $k$ features maps in the last convolutional layer: $A_1, A_2, \\dots, A_k$. Grad-CAM has to decide how important each of the $k$ feature map was to our class $c$ that we are interested in. We have to weight each pixel of each feature map with the gradient before we average over the feature maps.\n",
        "We are looking for weights $\\alpha^c_k$ that tell us how important is the pixel of each feature map, then we get the map by averaging the weighted features map:\n",
        "$$map = ReLU \\left(\\sum_k \\alpha^c_k A^k \\right)$$\n",
        "\n",
        "**Method**:\n",
        "1. Compute the prediction (i.e. *the forward pass*).\n",
        "2. Backpropagate the gradient of the *raw* class score of interest to the last convolution layer (before the FC layers). The gradients are set to zero for all classes except the desired class, which is set to 1.\n",
        "3. Weight each feature map with:\n",
        "$$\\alpha_k^c = \\frac 1 Z \\sum_i \\sum_j \\frac{\\partial y_{pred}}{\\partial A^k_{i,j}}$$\n",
        "4. Compute the heatmap:\n",
        "$$map = ReLU \\left(\\sum_k \\alpha^c_k A^k \\right)$$\n",
        "3. Scale the heatmap to the size of the input image.\n",
        "4. Vizualize the gradients.\n",
        "\n",
        "* **Hints**:\n",
        " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
        " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57XpgfiY_xUz"
      },
      "outputs": [],
      "source": [
        "def get_activations(mymodel, index, my_class_of_interest):\n",
        "\n",
        "    mymodel.eval()\n",
        "\n",
        "    activations = []\n",
        "    activations_grad = []\n",
        "\n",
        "    def forward_hook(layer, _, outputs_of_the_layer):\n",
        "        # TO COMPLETE\n",
        "        pass\n",
        "\n",
        "    def backward_hook(layer, _, outputs_of_the_layer):\n",
        "        # TO COMPLETE\n",
        "        pass\n",
        "\n",
        "    mymodel.layer4.register_forward_hook(forward_hook)\n",
        "    mymodel.layer4.register_backward_hook(backward_hook)\n",
        "\n",
        "    data = dataset[index][0].view(1, 3, 224, 224)\n",
        "    result = mymodel(data)\n",
        "    mymodel.zero_grad()\n",
        "\n",
        "    result[:, my_class_of_interest].backward(retain_graph=True)\n",
        "\n",
        "    return activations, activations_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNU7o92yEfpt"
      },
      "outputs": [],
      "source": [
        "def alpha_k(activations_grad, k):\n",
        "    return activations_grad[0][:, k, :, :].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTNoCOjuEhfG"
      },
      "outputs": [],
      "source": [
        "def create_map(activations, activations_grad):\n",
        "\n",
        "    n_feature_maps = activations[0].shape[1]\n",
        "    width = activations[0].shape[2]\n",
        "    alphas = [alpha_k(activations_grad,k) for k in range(n_feature_maps)]\n",
        "    heat_map = torch.zeros((1,1,width, width))\n",
        "    for k in range(n_feature_maps):\n",
        "        heat_map += # TO COMPLETE\n",
        "    heat_map = F.relu(heat_map)\n",
        "\n",
        "    #rescale the map\n",
        "    rescaled_heat_map = F.interpolate(heat_map, size = (224,224), mode = \"bilinear\")\n",
        "    rescaled_heat_map /= rescaled_heat_map.max()\n",
        "    return heat_map, rescaled_heat_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoR34O2tEi5Z"
      },
      "outputs": [],
      "source": [
        "def Grad_CAM(index_img, my_class_of_interest):\n",
        "    resnet34 = models.resnet34(pretrained=True)\n",
        "    act, act_grad = get_activations(resnet34, index_img, my_class_of_interest)\n",
        "    heat_map, rescaled_heat_map = create_map(act, act_grad)\n",
        "    return heat_map, rescaled_heat_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "BhWLSTFJEkRR",
        "outputId": "be38dab4-5c5d-4f3a-f725-0f805c566eb2"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
        "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
        "values, indices = torch.topk(output, 3)\n",
        "print(indices[0])\n",
        "top_class = indices[0][0]\n",
        "top_classes = [classes[x] for x in indices[0].numpy()]\n",
        "print(top_classes)\n",
        "img = transforms.CenterCrop(224)(\n",
        "    Image.open(dataset.imgs[index][0]).convert('RGB'))\n",
        "axs[0].imshow(img, alpha=1)\n",
        "axs[0].set_title('Picture {}'.format(index+1))\n",
        "\n",
        "heat_map, rescaled_heat_map = Grad_CAM(index, top_class)\n",
        "axs[1].imshow(heat_map[0, 0, :, :].detach().numpy())\n",
        "axs[2].imshow(img, alpha=1)\n",
        "axs[2].imshow(rescaled_heat_map.detach().numpy()[\n",
        "              0, 0, :, :], cmap=\"jet\", alpha=0.6)\n",
        "axs[2].set_title(\"Class {}\".format(top_classes[0]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "_gPhl3UNI_xy",
        "outputId": "b4ac9fdf-5f82-49fd-dc1c-ff5802cc19fc"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "\n",
        "some_class = 282  # class : tiger cat\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
        "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
        "\n",
        "img = transforms.CenterCrop(224)(\n",
        "    Image.open(dataset.imgs[index][0]).convert('RGB'))\n",
        "axs[0].imshow(img, alpha=1)\n",
        "axs[0].set_title('Picture {}'.format(index+1))\n",
        "\n",
        "heat_map, rescaled_heat_map = Grad_CAM(index, some_class)\n",
        "axs[1].imshow(heat_map[0, 0, :, :].detach().numpy())\n",
        "axs[2].imshow(img, alpha=1)\n",
        "axs[2].imshow(rescaled_heat_map.detach().numpy()[\n",
        "              0, 0, :, :], cmap=\"jet\", alpha=0.6)\n",
        "axs[2].set_title(\"Class {}\".format(classes[some_class]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_U1MCLfNgFA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
